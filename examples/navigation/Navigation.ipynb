{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Navigation\n",
    "\n",
    "---\n",
    "\n",
    "In this notebook, you will learn how to use the Unity ML-Agents environment for the first project of the [Deep Reinforcement Learning Nanodegree](https://www.udacity.com/course/deep-reinforcement-learning-nanodegree--nd893).\n",
    "\n",
    "### 1. Start the Environment\n",
    "\n",
    "We begin by importing some necessary packages.  If the code cell below returns an error, please revisit the project instructions to double-check that you have installed [Unity ML-Agents](https://github.com/Unity-Technologies/ml-agents/blob/master/docs/Installation.md) and [NumPy](http://www.numpy.org/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from unityagents import UnityEnvironment\n",
    "import numpy as np\n",
    "import logging\n",
    "import os\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we will start the environment!  **_Before running the code cell below_**, change the `file_name` parameter to match the location of the Unity environment that you downloaded.\n",
    "\n",
    "- **Mac**: `\"path/to/Banana.app\"`\n",
    "- **Windows** (x86): `\"path/to/Banana_Windows_x86/Banana.exe\"`\n",
    "- **Windows** (x86_64): `\"path/to/Banana_Windows_x86_64/Banana.exe\"`\n",
    "- **Linux** (x86): `\"path/to/Banana_Linux/Banana.x86\"`\n",
    "- **Linux** (x86_64): `\"path/to/Banana_Linux/Banana.x86_64\"`\n",
    "- **Linux** (x86, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86\"`\n",
    "- **Linux** (x86_64, headless): `\"path/to/Banana_Linux_NoVis/Banana.x86_64\"`\n",
    "\n",
    "For instance, if you are using a Mac, then you downloaded `Banana.app`.  If this file is in the same folder as the notebook, then the line below should appear as follows:\n",
    "```\n",
    "env = UnityEnvironment(file_name=\"Banana.app\")\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:unityagents:\n",
      "'Academy' started successfully!\n",
      "Unity Academy name: Academy\n",
      "        Number of Brains: 1\n",
      "        Number of External Brains : 1\n",
      "        Lesson number : 0\n",
      "        Reset Parameters :\n",
      "\t\t\n",
      "Unity brain name: BananaBrain\n",
      "        Number of Visual Observations (per agent): 0\n",
      "        Vector Observation space type: continuous\n",
      "        Vector Observation space size (per agent): 37\n",
      "        Number of stacked Vector Observation: 1\n",
      "        Vector Action space type: discrete\n",
      "        Vector Action space size (per agent): 4\n",
      "        Vector Action descriptions: , , , \n"
     ]
    }
   ],
   "source": [
    "env = UnityEnvironment(file_name=\"./Banana\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Environments contain **_brains_** which are responsible for deciding the actions of their associated agents. Here we check for the first brain available, and set it as the default brain we will be controlling from Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get the default brain\n",
    "brain_name = env.brain_names[0]\n",
    "brain = env.brains[brain_name]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Build the RL-Library\n",
    "The core classes and algorithms used to solve this project are packaged in a python module. The following 2 cells will first change the current directory to the root of the repository, then install the python package. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change Current Working Directory to the root of the repository."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/quentincangelosi/Documents/5. Other/2. DataScience/github/RL-Library'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cwd = os.path.abspath('')\n",
    "os.chdir(Path(cwd).parents[1])\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "[OPTIONAL]: If you haven't already, install the package requirements."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pandas==1.1.2 in /Users/quentincangelosi/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 1)) (1.1.2)\n",
      "Requirement already satisfied: numpy==1.19.2 in /Users/quentincangelosi/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 2)) (1.19.2)\n",
      "Collecting matplotlib==3.3.2 (from -r requirements.txt (line 3))\n",
      "  Using cached https://files.pythonhosted.org/packages/63/f0/c2c11e34d43f657df8ae05be5fa991200a2ed576e3694244a9dc766c14c3/matplotlib-3.3.2-cp37-cp37m-macosx_10_9_x86_64.whl\n",
      "Collecting seaborn==0.11.0 (from -r requirements.txt (line 4))\n",
      "  Using cached https://files.pythonhosted.org/packages/bc/45/5118a05b0d61173e6eb12bc5804f0fbb6f196adb0a20e0b16efc2b8e98be/seaborn-0.11.0-py3-none-any.whl\n",
      "Requirement already satisfied: torch==1.6.0 in /Users/quentincangelosi/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 7)) (1.6.0)\n",
      "Requirement already satisfied: gym==0.17.2 in /Users/quentincangelosi/anaconda3/lib/python3.7/site-packages (from -r requirements.txt (line 10)) (0.17.2)\n",
      "Requirement already satisfied: pytz>=2017.2 in /Users/quentincangelosi/anaconda3/lib/python3.7/site-packages (from pandas==1.1.2->-r requirements.txt (line 1)) (2019.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7.3 in /Users/quentincangelosi/anaconda3/lib/python3.7/site-packages (from pandas==1.1.2->-r requirements.txt (line 1)) (2.8.0)\n",
      "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.3 in /Users/quentincangelosi/anaconda3/lib/python3.7/site-packages (from matplotlib==3.3.2->-r requirements.txt (line 3)) (2.4.0)\n",
      "Requirement already satisfied: cycler>=0.10 in /Users/quentincangelosi/anaconda3/lib/python3.7/site-packages (from matplotlib==3.3.2->-r requirements.txt (line 3)) (0.10.0)\n",
      "Requirement already satisfied: pillow>=6.2.0 in /Users/quentincangelosi/anaconda3/lib/python3.7/site-packages (from matplotlib==3.3.2->-r requirements.txt (line 3)) (7.2.0)\n",
      "Requirement already satisfied: kiwisolver>=1.0.1 in /Users/quentincangelosi/anaconda3/lib/python3.7/site-packages (from matplotlib==3.3.2->-r requirements.txt (line 3)) (1.1.0)\n",
      "Collecting certifi>=2020.06.20 (from matplotlib==3.3.2->-r requirements.txt (line 3))\n",
      "  Using cached https://files.pythonhosted.org/packages/5e/c4/6c4fe722df5343c33226f0b4e0bb042e4dc13483228b4718baf286f86d87/certifi-2020.6.20-py2.py3-none-any.whl\n",
      "Requirement already satisfied: scipy>=1.0 in /Users/quentincangelosi/anaconda3/lib/python3.7/site-packages (from seaborn==0.11.0->-r requirements.txt (line 4)) (1.3.0)\n",
      "Requirement already satisfied: future in /Users/quentincangelosi/anaconda3/lib/python3.7/site-packages (from torch==1.6.0->-r requirements.txt (line 7)) (0.18.2)\n",
      "Requirement already satisfied: pyglet<=1.5.0,>=1.4.0 in /Users/quentincangelosi/anaconda3/lib/python3.7/site-packages (from gym==0.17.2->-r requirements.txt (line 10)) (1.5.0)\n",
      "Requirement already satisfied: cloudpickle<1.4.0,>=1.2.0 in /Users/quentincangelosi/anaconda3/lib/python3.7/site-packages (from gym==0.17.2->-r requirements.txt (line 10)) (1.2.1)\n",
      "Requirement already satisfied: six>=1.5 in /Users/quentincangelosi/anaconda3/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas==1.1.2->-r requirements.txt (line 1)) (1.12.0)\n",
      "Requirement already satisfied: setuptools in /Users/quentincangelosi/anaconda3/lib/python3.7/site-packages (from kiwisolver>=1.0.1->matplotlib==3.3.2->-r requirements.txt (line 3)) (50.3.0)\n",
      "Installing collected packages: certifi, matplotlib, seaborn\n",
      "  Found existing installation: certifi 2019.6.16\n",
      "\u001b[31mERROR: Cannot uninstall 'certifi'. It is a distutils installed project and thus we cannot accurately determine which files belong to it which would lead to only a partial uninstall.\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install the rl_library package."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obtaining file:///Users/quentincangelosi/Documents/5.%20Other/2.%20DataScience/github/RL-Library\n",
      "Requirement already satisfied: Click>=7.0 in /Users/quentincangelosi/anaconda3/lib/python3.7/site-packages (from rl-library==0.0.1) (7.0)\n",
      "Installing collected packages: rl-library\n",
      "  Found existing installation: rl-library 0.0.1\n",
      "    Uninstalling rl-library-0.0.1:\n",
      "      Successfully uninstalled rl-library-0.0.1\n",
      "  Running setup.py develop for rl-library\n",
      "Successfully installed rl-library\n"
     ]
    }
   ],
   "source": [
    "!pip install -e ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from rl_library.monitors import unity_monitor\n",
    "from rl_library.agents import DQAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Revert current working directory to examples/navigation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/Users/quentincangelosi/Documents/5. Other/2. DataScience/github/RL-Library/examples/navigation'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "os.chdir(cwd)\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Important Input Parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following are input parameters that can be changed to generate new solutions:\n",
    "\n",
    "    - hidden_layer_sizes: List of hidden layers sizes for the Deep-Q-Network (a standard feed-forward fully connected neural network architecture)\n",
    "    - options: List of options to improve the Deep Q Learning algorithm. Possible values:\n",
    "        - \"double-q-learning\": to enhance Deep Q Learning into Double Deep Q Learning\n",
    "        - \"prioritized-replay\": to enhance the experience replay into prioritized experience replay\n",
    "    - mode: Enable training or testing phase\n",
    "    - save_path: Where the results will be located"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_layer_sizes = [20, 15, 8]\n",
    "options = [\"double-q-learning\"]     # [\"double-q-learning\", \"prioritized-replay\"]\n",
    "mode = \"train\"                      # \"train\" or \"test\"\n",
    "save_path = f\"DDQN_\" + \"_\".join([str(sz) for sz in hidden_layer_sizes])\n",
    "os.makedirs(save_path, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initiate a logger to record logs of the simulations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Logger\n",
    "logger = logging.getLogger()\n",
    "handler = logging.FileHandler(f\"{save_path}/logs_navigation_{pd.Timestamp.utcnow().value}.log\")\n",
    "stream_handler = logging.StreamHandler()\n",
    "logger.addHandler(handler)\n",
    "logger.addHandler(stream_handler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Examine the State and Action Spaces\n",
    "\n",
    "The simulation contains a single agent that navigates a large environment.  At each time step, it has four actions at its disposal:\n",
    "- `0` - walk forward \n",
    "- `1` - walk backward\n",
    "- `2` - turn left\n",
    "- `3` - turn right\n",
    "\n",
    "The state space has `37` dimensions and contains the agent's velocity, along with ray-based perception of objects around agent's forward direction.  A reward of `+1` is provided for collecting a yellow banana, and a reward of `-1` is provided for collecting a blue banana. \n",
    "\n",
    "Run the code cell below to print some information about the environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Number of agents: 1\n",
      "Number of agents: 1\n",
      "INFO:root:Number of actions: 4\n",
      "Number of actions: 4\n",
      "INFO:root:States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "States look like: [1.         0.         0.         0.         0.84408134 0.\n",
      " 0.         1.         0.         0.0748472  0.         1.\n",
      " 0.         0.         0.25755    1.         0.         0.\n",
      " 0.         0.74177343 0.         1.         0.         0.\n",
      " 0.25854847 0.         0.         1.         0.         0.09355672\n",
      " 0.         1.         0.         0.         0.31969345 0.\n",
      " 0.        ]\n",
      "INFO:root:States have length: 37\n",
      "States have length: 37\n"
     ]
    }
   ],
   "source": [
    "# reset the environment\n",
    "env_info = env.reset(train_mode=True)[brain_name]\n",
    "\n",
    "# number of agents in the environment\n",
    "logger.info(f'Number of agents: {len(env_info.agents)}')\n",
    "\n",
    "# number of actions\n",
    "action_size = brain.vector_action_space_size\n",
    "logger.info(f'Number of actions: {action_size}')\n",
    "\n",
    "# examine the state space\n",
    "state = env_info.vector_observations[0]\n",
    "logger.info(f'States look like: {state}')\n",
    "state_size = len(state)\n",
    "logger.info(f'States have length: {state_size}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Train the Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the following cell, the agent is going to be trained to solve the navigation problem. \n",
    "More information about the classes and functions used can be found in Report.md"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "INFO:root:Initialized model: Sequential(\n",
      "  (fc0): Linear(in_features=37, out_features=20, bias=True)\n",
      "  (relu0): ReLU()\n",
      "  (fc1): Linear(in_features=20, out_features=15, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=15, out_features=8, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (output): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "Initialized model: Sequential(\n",
      "  (fc0): Linear(in_features=37, out_features=20, bias=True)\n",
      "  (relu0): ReLU()\n",
      "  (fc1): Linear(in_features=20, out_features=15, bias=True)\n",
      "  (relu1): ReLU()\n",
      "  (fc2): Linear(in_features=15, out_features=8, bias=True)\n",
      "  (relu2): ReLU()\n",
      "  (output): Linear(in_features=8, out_features=4, bias=True)\n",
      "  (softmax): Softmax(dim=1)\n",
      ")\n",
      "INFO:root:Using Double Q-Learning.\n",
      "Using Double Q-Learning.\n",
      "/Users/quentincangelosi/Documents/5. Other/2. DataScience/github/RL-Library/rl_library/agents/dqn_agent.py:308: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
      "  experiences = np.array(self.memory)[experiences_ids]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 99\tAverage Score: 0.24, DQN Avg. Loss: 3.03e-03, Last Score: 0.00, eps: 0.6119"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 100\tAverage Score: 0.24, DQN Avg. Loss: 3.92e-03, Last Score: 0.00\n",
      "Episode 100\tAverage Score: 0.24, DQN Avg. Loss: 3.92e-03, Last Score: 0.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 199\tAverage Score: -0.04, DQN Avg. Loss: 7.52e-03, Last Score: 0.00, eps: 0.377"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 200\tAverage Score: -0.03, DQN Avg. Loss: 5.73e-03, Last Score: 1.00\n",
      "Episode 200\tAverage Score: -0.03, DQN Avg. Loss: 5.73e-03, Last Score: 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 299\tAverage Score: 3.09, DQN Avg. Loss: 1.89e-02, Last Score: 3.00, eps: 0.2226"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 300\tAverage Score: 3.14, DQN Avg. Loss: 1.93e-02, Last Score: 6.00\n",
      "Episode 300\tAverage Score: 3.14, DQN Avg. Loss: 1.93e-02, Last Score: 6.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 399\tAverage Score: 6.39, DQN Avg. Loss: 2.01e-02, Last Score: 10.00, eps: 0.14"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 400\tAverage Score: 6.34, DQN Avg. Loss: 2.02e-02, Last Score: 1.00\n",
      "Episode 400\tAverage Score: 6.34, DQN Avg. Loss: 2.02e-02, Last Score: 1.00\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 499\tAverage Score: 7.43, DQN Avg. Loss: 2.00e-02, Last Score: 4.00, eps: 0.089"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Episode 500\tAverage Score: 7.50, DQN Avg. Loss: 2.20e-02, Last Score: 8.00\n",
      "Episode 500\tAverage Score: 7.50, DQN Avg. Loss: 2.20e-02, Last Score: 8.00\n",
      "INFO:root:\n",
      "Saving model to DDQN_20_15_8\n",
      "\n",
      "Saving model to DDQN_20_15_8\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Episode 537\tAverage Score: 7.72, DQN Avg. Loss: 2.38e-02, Last Score: 1.00, eps: 0.077"
     ]
    }
   ],
   "source": [
    "# Training\n",
    "if mode == \"train\":\n",
    "    agent = DQAgent(state_size=state_size, action_size=action_size,\n",
    "                    hidden_layer_sizes=hidden_layer_sizes, options=options)\n",
    "\n",
    "    scores = unity_monitor.run(env, agent, brain_name, save_every=500, save_path=save_path)\n",
    "    logger.info(\"Average Score last 100 episodes: {}\".format(np.mean(scores[-100:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "from matplotlib.colors import ListedColormap, LinearSegmentedColormap\n",
    "import seaborn as sb\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_rows', None)\n",
    "sb.set()\n",
    "sb.set_style(\"darkgrid\", {\"axes.facecolor\": \".9\"})\n",
    "sb.despine()\n",
    "sb.set_context(\"talk\")\n",
    "\n",
    "# Let's compare different solutions!\n",
    "\n",
    "solutions_path = [save_path,]\n",
    "df = pd.DataFrame()\n",
    "for path in solutions_path:\n",
    "    fn = f\"{path}/scores.pickle\"\n",
    "    if os.path.exists(fn):\n",
    "        with open(fn, \"rb\") as f:\n",
    "            scores = pickle.load(f)\n",
    "    _df = pd.DataFrame({\"score\": scores})\n",
    "    _df[\"name\"] = path\n",
    "    _df[\"episode\"] = _df.index + 1\n",
    "    _df[\"rolling_score_100\"] = _df[\"score\"].rolling(100).mean()\n",
    "    df = df.append(_df, ignore_index=True)\n",
    "\n",
    "plt.figure(figsize=(15, 8))\n",
    "sb.lineplot(\"episode\", \"rolling_score_100\", hue=\"name\", data=df)\n",
    "plt.axhline(13, c=\"k\", ls=\"--\", label=\"Objective: 13\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Test the Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing\n",
    "mode = \"test\"\n",
    "agent = DQAgent.load(filepath=save_path, mode=\"test\")\n",
    "scores = unity_monitor.run(env, agent, brain_name, n_episodes=10, length_episode=1e6, mode=\"test\")\n",
    "logger.info(f\"Test Score over {len(scores)} episodes: {np.mean(scores)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# When finished, you can close the environment.\n",
    "logger.info(\"Closing...\")\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl",
   "language": "python",
   "name": "rl"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
